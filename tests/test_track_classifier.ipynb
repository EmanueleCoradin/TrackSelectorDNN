{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming your model code is in ../models/\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.track_classifier import TrackClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTrackDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generates random hit-level data grouped by track.\n",
    "    Each track has 5â€“15 hits, each hit has `hit_input_dim` features.\n",
    "    Each track also has `track_feat_dim` features and a binary label.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_tracks=2000, hit_input_dim=8, track_feat_dim=4, max_hits=15):\n",
    "        super().__init__()\n",
    "        self.hit_input_dim = hit_input_dim\n",
    "        self.track_feat_dim = track_feat_dim\n",
    "\n",
    "        self.tracks = []\n",
    "        self.hits = []\n",
    "        self.batch_idx = []\n",
    "        self.labels = []\n",
    "\n",
    "        for track_id in range(n_tracks):\n",
    "            n_hits = np.random.randint(5, max_hits)\n",
    "            hits = np.random.randn(n_hits, hit_input_dim).astype(np.float32)\n",
    "            track_feats = np.random.randn(track_feat_dim).astype(np.float32)\n",
    "            label = np.random.randint(0, 2)\n",
    "\n",
    "            self.hits.append(hits)\n",
    "            self.tracks.append(track_feats)\n",
    "            self.labels.append(label)\n",
    "            self.batch_idx.append(np.full(n_hits, track_id, dtype=np.int64))\n",
    "\n",
    "        # Flatten for convenience\n",
    "        self.all_hits = np.concatenate(self.hits, axis=0)\n",
    "        self.all_batch_idx = np.concatenate(self.batch_idx, axis=0)\n",
    "        self.all_tracks = np.stack(self.tracks, axis=0)\n",
    "        self.all_labels = np.array(self.labels, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return per-track aggregated view\n",
    "        mask = self.all_batch_idx == idx\n",
    "        return (\n",
    "            torch.tensor(self.all_hits[mask]),\n",
    "            torch.tensor(self.all_tracks[idx]),\n",
    "            torch.tensor(idx),\n",
    "            torch.tensor(self.all_labels[idx])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    hits, tracks, batch_ids, labels = zip(*batch)\n",
    "    hit_features = torch.cat(hits, dim=0)\n",
    "    track_features = torch.stack(tracks, dim=0)\n",
    "    batch_indices = torch.cat([\n",
    "        torch.full((len(h),), i, dtype=torch.long)\n",
    "        for i, h in enumerate(hits)\n",
    "    ])\n",
    "    labels = torch.stack(labels).float()\n",
    "    return hit_features, track_features, batch_indices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DummyTrackDataset(n_tracks=500)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = TrackClassifier(\n",
    "    hit_input_dim=8,\n",
    "    track_feat_dim=4,\n",
    "    latent_dim=16,\n",
    "    pooling_type=\"softmax\",\n",
    "    netA_hidden_dim=32,\n",
    "    netA_hidden_layers=2,\n",
    "    netB_hidden_dim=64,\n",
    "    netB_hidden_layers=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for hit_features, track_features, batch_indices, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        hit_features = hit_features.to(device)\n",
    "        track_features = track_features.to(device)\n",
    "        batch_indices = batch_indices.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(hit_features, track_features, batch_indices)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(labels)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}: loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    hit_features, track_features, batch_indices, labels = next(iter(train_loader))\n",
    "    hit_features, track_features, batch_indices = (\n",
    "        hit_features.to(device), track_features.to(device), batch_indices.to(device)\n",
    "    )\n",
    "    preds = model(hit_features, track_features, batch_indices)\n",
    "    print(\"Predictions:\", preds[:10].cpu().numpy())\n",
    "    print(\"Labels:\", labels[:10].numpy())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
